{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "day-4-5-transformer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMs/EYbjQOz75vhZblD4+Kb"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Adnxs5kDc2pd"
      },
      "source": [
        "# (day4) Section 5: Transformer\n",
        "\n",
        "本書は、「深層学習後編（day4）レポート」の、「Section 5: Transformer」についてのものです。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_jMHYWudCh3"
      },
      "source": [
        "## 1. 要点まとめ\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<!-- - Seq2seq\n",
        "- page. 19\n",
        "  - RNN\n",
        "  - 言語モデル\n",
        "  - RNN x 言語モデル\n",
        "  - Encoder - Decoder\n",
        "  - 実装 -> \"実装演習\" で確認 -->\n"
      ],
      "metadata": {
        "id": "0GLqHNEehHaA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<!-- \n",
        "- Transformer - Self-Attention ( 事故注意結合 )\n",
        "- page. 23\n",
        "  - ニューラル機械翻訳の問題点\n",
        "  - Attention\n",
        "  - Transformer\n",
        "  - 主要モジュール\n",
        "    - Attention\n",
        "      - Source Target Attention\n",
        "      - Self-Attention\n",
        "    - Transformer-Encoder\n",
        "    - Self-Attention\n",
        "    - Position-Wise Feed-Forward-Networks\n",
        "    - Scaled dot product attention\n",
        "    - Multi-head attention\n",
        "    - Decoder\n",
        "      - Self-Attention\n",
        "      - Encoder-Decoder attention\n",
        "    - Add & Norm\n",
        "      - Add ( Residual Connection )\n",
        "      - Norm ( Layer Normarization )\n",
        "    - Position Encoding\n",
        "  - 実装 -> \"実装演習\" で確認 -->\n"
      ],
      "metadata": {
        "id": "r40jfanIi5eY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- NOTE: https://arxiv.org/pdf/1409.3215.pdf\n"
      ],
      "metadata": {
        "id": "1eT4BzoHh56m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Seq2seq ( Encoder-Decoder モデル )\n",
        "\n",
        "Transformer を理解するために必要な材料としての、 Seq2seq 。\n",
        "\n",
        "Encoder-Decoder モデルと呼ばれ、入力系列を Encode して内部状態に変換。\n",
        "その内部状態を Decode して新たな表現を作成する。\n",
        "\n",
        "Seq2seq の中の材料として、\n",
        "\n",
        "- RNN\n",
        "- 言語モデル\n"
      ],
      "metadata": {
        "id": "O-5s5lvD7m_o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### RNN ( Recurrent Neural Network )\n",
        "\n",
        "系列データの、前の時刻の出力を現在の入力に、再帰的に計算。\n",
        "\n",
        "系列データの全て ( 例えば、一文 ) を、1つの内部状態ベクトルで表現。\n"
      ],
      "metadata": {
        "id": "PJ1mfK1E86Hy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 言語モデル\n",
        "\n",
        "単語の並び ( 1, ... m-1, m ) を、事後確率で表現。\n",
        "\n",
        "$$\n",
        "P(w_1, \\cdots, w_m)\n",
        "=\\prod_{i=1}~m\n",
        "P(w_i \\mid w_1, \\cdots, w_{i-1})\n",
        "$$\n",
        "\n",
        "それぞれの単語の同時確率を求める。\n",
        "\n",
        "$$\n",
        "argmax_{w \\in V} P(w_1, \\cdots, w_v)\n",
        "$$\n",
        "\n",
        "( V は全ての単語 )\n",
        "\n",
        "RNN を使用して、各時点で度の単語を使えば自然 ( 事後確率最大 ) かを出力できる。\n"
      ],
      "metadata": {
        "id": "hjEZpqRm7uxD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Encoder - Decoder\n",
        "\n",
        "##### Encoder\n",
        "\n",
        "Source から Embed 表現にした単語の入力を Encoder RNN で学習する。\n",
        "\n",
        "Encoder の学習方法結果が、内部状態ベクトルとして出力される。\n",
        "\n",
        "##### Decoder\n",
        "\n",
        "Decoder RNN は、Encoder の出力の、内部状態ベクトルから、\n",
        "新しい表現の各単語の事後確率を出力する。\n",
        "( Softmax 活性化関数で、各単語の確率を計算する )\n",
        "\n",
        "##### 学習\n",
        "\n",
        "Decoder の出力と、訓練データの正解から誤差関数を使用すれば、\n",
        "教師あり学習として、 Encoder から Decoder まで End2end で学習できる。\n",
        "\n"
      ],
      "metadata": {
        "id": "JcN0ysJ0TxLr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transformer - Self-Attention ( 事故注意機構 )"
      ],
      "metadata": {
        "id": "608xzux-7rdl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ニューラル機械翻訳の問題点\n",
        "\n",
        "翻訳元の文を 1 つのベクトルで表現しているため、文長が長くなると表現力が不足する。\n"
      ],
      "metadata": {
        "id": "BmgiTdMVdHHv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Attention\n",
        "\n",
        "文長が長くなると表現力が不足する問題点を解決するために、翻訳先の各単語を選択するときに、翻訳元の文中の各単語の隠れ状態 ( ベクトル ) を利用する。\n",
        "\n",
        "<!-- - TODO: 隠れ状態? -->\n",
        "\n",
        "翻訳元の文中の各単語の隠れ状態の、加重平均を、\n",
        "\n",
        "$$\n",
        "c_i = \\sum_{j=1}^{T_{x}} \\alpha_{ij} h_j \\cdot\n",
        "$$\n",
        "\n",
        "重み ( 全て足すと 1 ) は、 FFNN で求める。\n",
        "\n",
        "$$\n",
        "\\alpha_{ij} = \\frac{\\exp(e_{ij})}\n",
        "{\\sum_{k=1}^{T_x} \\exp(e_{ik})}, \\\\\n",
        "e_{ij} = a(s_{i-1}, h_j)\n",
        "$$\n"
      ],
      "metadata": {
        "id": "lA7gMPjydW2u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Attention は、key を索引にした辞書オブジェクトの様に、 query で key に一致する value の、 Attention weight value を取り出すことができる。\n",
        "\n",
        "<!-- - TODO: 図の Attention weight value?</br>\n",
        "  valuet が、 Attention weight value のこと? -->\n",
        "\n",
        "- NOTE: [Multi-head attention mechanism: \"queries\", \"keys\", and \"values,\" over and over again - Data Science Blog](https://data-science-blog.com/blog/2021/04/07/multi-head-attention-mechanism/)<br/>\n",
        "  > Please remember this mantra of attention mechanism: “you compare the ‘query’ with the ‘keys’ and get scores/weights for the ‘values.’"
      ],
      "metadata": {
        "id": "249zhxGj7HH2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Transformer\n",
        "\n",
        "RNN を使わずに Attention を使用して、少ない計算量を実現。"
      ],
      "metadata": {
        "id": "zl4qQ-Iu6q9e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 主要モジュール\n",
        "\n",
        "以降に登場する主要モジュールは、以下の通り。\n",
        "\n",
        "- Attention\n",
        "  - Source Target Attention\n",
        "  - Self-Attention\n",
        "- Transformer-Encoder\n",
        "- Self-Attention\n",
        "- Position-Wise Feed-Forward-Networks\n",
        "- Scaled dot product attention\n",
        "- Multi-head attention\n",
        "- Decoder\n",
        "  - Self-Attention\n",
        "  - Encoder-Decoder attention\n",
        "- Add & Norm\n",
        "  - Add ( Residual Connection )\n",
        "  - Norm ( Layer Normarization )\n",
        "- Position Encoding\n"
      ],
      "metadata": {
        "id": "mE7g7vls79GS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Attention\n",
        "\n",
        "注意機構 ( Attention ) の種類は以下の 2 つ。\n",
        "\n",
        "- Source Target Attention\n",
        "\n",
        "$$\n",
        "\\sigma(Query=Target, Key=Source) Value=Source\n",
        "$$\n",
        "\n",
        "- Self-Attention\n",
        "\n",
        "$$\n",
        "\\sigma(Query=Source, Key=Source) Value=Source\n",
        "$$\n"
      ],
      "metadata": {
        "id": "OwVMe9e38W1i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Transformer-Encoder\n",
        "\n",
        "6層と、自己注意機構 ( Attention ) と Feed Forward NN から成る。\n"
      ],
      "metadata": {
        "id": "epLuOiF-82kf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Self-Attention\n",
        "\n",
        "例として、 $ i = 1, 2, 3 $\n",
        "\n",
        "1. 入力 $ x_i $ と 重み $ W^k $ から、 key $ k_i $ 、<br/>\n",
        "  入力 $ x_i $ と 重み $ W^v $ から、 key $ v_i $ を得る。\n",
        "\n",
        "2. query と key の内積 $ q_i \\cdot k_i $ に、\n",
        "  softmax 関数を適用して、確率を求める。<br/>\n",
        "  ( ここではこれを、 $ p_i $ とする )\n",
        "\n",
        "3. Feed Forward Network で、\n",
        "\n",
        "$$\n",
        "z_1 = \\sum_{i=1}^3 p_i v_i \\\\\n",
        "output = FFN(z_1)\n",
        "$$\n"
      ],
      "metadata": {
        "id": "1r46Mahf9n3X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Position-Wise Feed-Forward-Networks\n",
        "\n",
        "Attention 層の出力を決定する、 2 備全結合 NN。\n",
        "\n",
        "線形変換 ( $ W_1 $ ) -> ReLu -> 線形変換 ( $ W_2 $ )\n",
        "\n",
        "$$\n",
        "FFN(\\textbf{x}) = \\max(0, \\textbf{x} W_1 + \\textbf{b}_1) W_2 + \\textbf{b}_2\n",
        "$$\n"
      ],
      "metadata": {
        "id": "6Kq0Uajv_sRJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Scaled dot product attention\n",
        "\n",
        "全単語に関する Attention をまとめて計算。\n",
        "\n",
        "$$\n",
        "Attention(\\textbf{Q}, \\textbf{K}, \\textbf{V})\n",
        "=\n",
        "softmax(\n",
        "  \\frac{\\textbf{Q} \\textbf{K}^T}\n",
        "  {\n",
        "    \\sqrt{d_k}\n",
        "  } \\textbf{V}\n",
        ")\n",
        "$$\n"
      ],
      "metadata": {
        "id": "E9yOYlC2AnuU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Multi-head attention\n",
        "\n",
        "重みパラメタの異なる８個の Scaled dot product attention ( ヘッド ) の出力を連結。\n",
        "\n",
        "```python\n",
        "        # 各ヘッドの結果を連結(p16左側`Concat`)\n",
        "        # torch.splitでbatch_sizeごとのn_head個のテンソルに分割\n",
        "        outputs = torch.split(outputs, batch_size, dim=0)  # (batch_size, len_q, d_model) * n_head\n",
        "        # dim=-1で連結\n",
        "        outputs = torch.cat(outputs, dim=-1)  # (batch_size, len_q, d_model*n_head)\n",
        "```\n"
      ],
      "metadata": {
        "id": "7I-kieGMBVht"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Decoder\n",
        "\n",
        "Encoder 同様、 6 層。\n"
      ],
      "metadata": {
        "id": "DpbX6GAkCMS4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Self-Attention\n",
        "\n",
        "未来の情報を見ないようにマスクしている。\n"
      ],
      "metadata": {
        "id": "w2hOTx3gCS7d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Encoder-Decoder attention\n",
        "\n",
        "Encoder の出力からの attention を Decoder の Multi-Head Attention で受ける。\n",
        "\n",
        "<!-- - TODO: 図のどこのことか? -->\n"
      ],
      "metadata": {
        "id": "lELUOxYVCd84"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Add & Norm\n"
      ],
      "metadata": {
        "id": "dkUkpGIqC5sB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Add ( Residual Connection )\n",
        "\n",
        "入出力の差分を学習して、学習・テストエラーを低減させる。\n",
        "\n",
        "```python\n",
        "        # residual connectionのための入力 出力に入力をそのまま加算する\n",
        "        residual = q\n",
        "```\n"
      ],
      "metadata": {
        "id": "4Oa_GYFMDAVo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Norm ( Layer Normarization )\n",
        "\n",
        "学習の高速化のための、正則化。\n",
        "\n",
        "- NOTE: (day3) Section 3: 軽量化 - 高速化技術\n"
      ],
      "metadata": {
        "id": "7GRNziO1DED8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Position Encoding\n",
        "\n",
        "RNN を使用しない代わりの、単語列語順情報。\n",
        "\n",
        "$$\n",
        "PE_{(pos, 2i)} = \\sin{(\n",
        "  \\frac{pos}\n",
        "  {10000^{2i / 512}}\n",
        ")} \\\\\n",
        "PE_{(pos, 2i + 1)} = \\cos{(\n",
        "  \\frac{pos}\n",
        "  {10000^{2i / 512}}\n",
        ")}\n",
        "$$\n",
        "\n",
        "- pos: 単語の位置\n",
        "- i: 成分の次元\n",
        "- 512: モデルの次元\n"
      ],
      "metadata": {
        "id": "70lDZXBeETrm"
      }
    }
  ]
}