{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "day-3-7-attention-mechanism.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyP8Xky1IfxMSsfWmvJfiHtL"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Adnxs5kDc2pd"
      },
      "source": [
        "# (day3) Section 7: Attention Mechanism\n",
        "\n",
        "本書は、「深層学習後編（day3）レポート」の、「Section 7: Attention Mechanism」についてのものです。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_jMHYWudCh3"
      },
      "source": [
        "## 1. 要点まとめ\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### seq2seq の課題\n",
        "\n",
        "seq2seq の隠れ層の大きさは決まっていて、長い文章を与えられても、 hidden sate のサイズは固定である。\n"
      ],
      "metadata": {
        "id": "l5X1QuSAOvHY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Attention Mechanism による解決\n",
        "\n",
        "文章の中でも重要な単語を見付けられる機構を追加する。\n",
        "単語同士で関連度が高い単語を、重要とみなす。\n",
        "\n",
        "中間層 ( 隠れ層 ) の大きさが一定でも、重要な単語を選択することができる。\n"
      ],
      "metadata": {
        "id": "EteGFwmLPJoI"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dn0nkaSx8-Em"
      },
      "source": [
        "## 2. 確認テスト\n",
        "\n",
        "以降の \"page. \" は、講義資料のページの番号です。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### page. 137\n",
        "\n",
        "- RNN と word2vec<br>\n",
        "  RNN は時系列に学習を行う NN 。 word2vec は単語のベクトル表現を得る手法で、 RNN ( seq2seq ) の中で使われるもの。<br>\n",
        "  よって、適用する部分が違う。\n",
        "\n",
        "- seq2seq と Attention<br>\n",
        "  seq2seq は時系列データ ( 文 ) の、文脈表現を得て、そこから新たな表現を作成するもの。機械対話や機械翻訳に使用。<br>\n",
        "  Attention は、 seq2seq において長い文章に対応するための手法。<br>\n",
        "  こちらも、適用する部分が違う。\n"
      ],
      "metadata": {
        "id": "yZbecsCxQVmk"
      }
    }
  ]
}