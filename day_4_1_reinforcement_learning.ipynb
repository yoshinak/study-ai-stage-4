{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "day-4-1-reinforcement-learning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPnFMDjx3LmPcd0CzkCl6Z/"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Adnxs5kDc2pd"
      },
      "source": [
        "# (day4) Section 1: 強化学習\n",
        "\n",
        "本書は、「深層学習後編（day4）レポート」の、「Section 1: 強化学習」についてのものです。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_jMHYWudCh3"
      },
      "source": [
        "## 1. 要点まとめ\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<!-- - page. 4 - 16\n",
        "\n",
        "- 概要\n",
        "- イメージ\n",
        "- 応用例\n",
        "- 探索と利用のトレードオフ\n",
        "  - 不完全な知識\n",
        "  - 探索と利用\n",
        "- 強化学習イメージ ( 構成要素 )\n",
        "- 強化学習の差分 ( 他の学習との違い )\n",
        "- 強化学習の歴史 ( 進展のきっかけとなった 2 つの手法の組み合わせ )\n",
        "- 価値関数\n",
        "- 方策関数\n",
        "- 方策勾配法 -->\n"
      ],
      "metadata": {
        "id": "9wtHxCTMZ_zu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 強化学習イメージ ( 構成要素 )\n",
        "\n",
        "強化学習とは、行動の結果与えられる報酬をもとに、\n",
        "目的に向かって ( 報酬の最大化 ) 、行動を改善する仕組み。\n",
        "\n",
        "- エージェント<br>\n",
        "ある環境の中で、長期的に報酬を最大化する行動を選択する。<br>\n",
        "強化学習の目標は、このエージェントを作る ( 学習する ) こと。\n",
        "  - 方策 TT<br/>\n",
        "    学習する。\n",
        "    方策関数: TT(s, a)\n",
        "  - 価値 V\n",
        "    学習する。<br/>\n",
        "    行動価値関数: Q(s, a)\n",
        "    - s, a: state ( 状態 ), action ( 行動 )\n",
        "\n",
        "- 環境\n",
        "  - 状態 S\n",
        "\n",
        "- 行動<br>\n",
        "  エージェントが方策に従って選択した行動。\n",
        "\n",
        "- 観測<br>\n",
        "  エージェントが方策を判断するために、環境 ( 状態 ) の情報 ( 変化 ) を得ること。\n",
        "\n",
        "- 報酬<br>\n",
        "  行動の結果。エージェントが判断する価値の元になる情報。\n"
      ],
      "metadata": {
        "id": "qMUm7LxndcWM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 探索と利用のトレードオフ\n"
      ],
      "metadata": {
        "id": "5g2Z7-48flJX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 不完全な知識\n",
        "\n",
        "強化学習では、訓練データのある教師あり/なし学習と違い、\n",
        "環境について事前知識を持たない。\n",
        "\n",
        "不完全な知識しか持たないため、<br>\n",
        "**探索** と **利用** の、\n",
        "行動しながらデータを収集して、最適な行動を見つける。<br>\n",
        "( 学習する )\n"
      ],
      "metadata": {
        "id": "1PHEhbx5fnlH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 探索と利用\n",
        "\n",
        "探索と利用は、トレードオフの関係となる。バランスの取れた学習を試行する。\n",
        "\n",
        "- 利用<br>\n",
        "  - モチベーション<br>\n",
        "    過去のデータでベストとされる行動をとり続けたい。\n",
        "  - やり過ぎると<br>\n",
        "    他にもっと良い行動を見つけることができない。<br>\n",
        "    ( 探索が不足してしまう )\n",
        "    \n",
        "\n",
        "- 探索<br>\n",
        "  - モチベーション<br>\n",
        "    未知の行動の中から、より良い行動を見つけたい。\n",
        "  - やり過ぎると<br>\n",
        "    過去の経験が不足してしまう。\n",
        "    ( 利用不足 )\n"
      ],
      "metadata": {
        "id": "69bl9pzdr2l2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- NOTE\n",
        "  - [Exploration vs. Exploitation in Reinforcement Learning](https://www.manifold.ai/exploration-vs-exploitation-in-reinforcement-learning)<br>\n",
        "  > EXPLORATION VS EXPLOITATION TRADEOFF\n",
        "  - [強化学習での「利用と探索のトレードオフ」と対処法 - Qiita](https://qiita.com/kanta_yamaoka/items/d20c9e49615196804ba7)\n"
      ],
      "metadata": {
        "id": "Ab1YEAuy40IZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 強化学習の差分 ( 他の学習との違い )\n",
        "\n",
        "訓練データのある教師あり/なし学習では、データから学習および予測を行うことが目標。\n",
        "\n",
        "強化学習では、 ( 訓練データのない環境で最適な行動を行う ) 優れた方策を見付けることが目標。\n"
      ],
      "metadata": {
        "id": "0yp-qG0xsidZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 強化学習の歴史 ( 進展のきっかけとなった 2 つの手法の組み合わせ )\n",
        "\n",
        "以下 2 つの手法を組み合わせることによって、近年、技術が進展した。\n",
        "\n",
        "- Q学習<br>\n",
        "  行動価値関数を、行動する毎に更新することにより、学習を進める手法。<br>\n",
        "  ( 方策勾配法 )\n",
        "\n",
        "- 関数近似法<br>\n",
        "  価値関数や方策関数を、関数近似する手法。<br>\n",
        "  以前は、条件に対して行動を対応付けた表形式 ( ルールベース ) だった。<br>\n",
        "  入力から出力を得る関数の形に近似させた。よって、 NN を適用できるようになった。\n"
      ],
      "metadata": {
        "id": "eXU-u7rqtO7o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 価値関数\n",
        "\n",
        "\"強化学習イメージ ( 構成要素 )\" において、\"価値 V\" を計算するもの。\n",
        "\n",
        "価値を表す関数の種類は、以下の 2 種類。\n",
        "\n",
        "- 状態価値関数<br>\n",
        "  ある状態の価値に注目する。\n",
        "\n",
        "- 行動価値関数 ( \"強化学習イメージ ( 構成要素 )\": $ Q(s, a) $ )<br>\n",
        "  状態と、エージェントがとった行動を組み合わせた価値に注目する。\n",
        "\n",
        "Q学習では、行動価値関数を使う。\n"
      ],
      "metadata": {
        "id": "PJCUZwk0uhVL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 方策関数\n",
        "\n",
        "方策ベースの強化学習手法において、<br>\n",
        "ある状態で、どの様な行動を採るのかの確率を与える関数。<br>\n",
        "エージェントが取る行動を決定する。\n"
      ],
      "metadata": {
        "id": "GOHHACkKwQGe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- NOTE: 方策ベース **ではない** 強化学習手法もあり。\n",
        "  - [強化学習とは(reinforcement learning)　価値ベースの手法と方策ベースの手法の違い～制御工学の基礎あれこれ～](http://arduinopid.web.fc2.com/N66.html)<br/>\n",
        "  > 価値ベースの手法:Value-Based Method\n"
      ],
      "metadata": {
        "id": "Qouh3RE3FkNM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<!-- 方策関数の数式は、以下の通り。\n",
        "\n",
        "$$\n",
        "\\pi(s) = a\n",
        "$$ -->\n"
      ],
      "metadata": {
        "id": "ZDH1Kqtl-lR_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 関数の関係\n",
        "\n",
        "エージェントは、方策に基づいて行動する。\n",
        "\n",
        "- 方策関数<br/>\n",
        "  利用 or 探索 など、その瞬間での行動をどうするかを決める。<br/>\n",
        "  ( V や Q を基にどういう行動をとるか? )<br>\n",
        "  $\n",
        "  \\pi(s, a)\n",
        "  $\n",
        "\n",
        "- 価値関数<br/>\n",
        "  ゴールまで今の方策を続けたときの報酬の予測値が得られる。<br/>\n",
        "  ( やり続けたら最終的にどうなるか? )\n",
        "  - 状態関数<br/>\n",
        "  $ V^{\\pi}(s) $\n",
        "  - 状態 + 行動関数<br/>\n",
        "  $ Q^{\\pi}(s, a) $\n"
      ],
      "metadata": {
        "id": "gT0zoBB4-3No"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- NOTE: [強化学習の基本的な考え方 - Qiita](https://qiita.com/qiita_kuru/items/2c00a81b4b26bf9ad210)<br>\n",
        "  > 局面sにおいて、ある戦略に基づいて行動aを起こす確率\n",
        "\n",
        "$$\n",
        "\\pi( a \\mid s )\n",
        "$$"
      ],
      "metadata": {
        "id": "MitDs5OZ98SA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 方策勾配法\n"
      ],
      "metadata": {
        "id": "YbgweLCdw5Pm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 方策反復法\n",
        "\n",
        "方策をモデル化して最適化する手法。\n",
        "\n",
        "- 方策勾配法<br/>\n",
        "  方策関数の NN を以下とすると、\n",
        "$$\n",
        "\\pi(s, a \\mid \\theta)\\\\\n",
        "$$\n",
        "方策勾配法は、\n",
        "$$\n",
        "\\theta^{(t+1)} = \\theta^{(t)} + \\epsilon \\nabla J (\\theta)\n",
        "$$\n",
        "  - $ \\theta $: 強化学習における NN の重み。<br/>\n",
        "    ( 今までの講義の $ \\textbf{W} $ に相当 )\n",
        "  - $ t $: 時刻\n",
        "  - $ J $<br>\n",
        "    期待収益。 NN での誤差関数。方策の良さを表す。\n",
        "\n",
        "今までの NN では、局所最適値 ( 傾きが最小 ) を求めていたが、<br/>\n",
        "強化学習では、報酬の最大化が目標であるから、 $ + $ となる。\n"
      ],
      "metadata": {
        "id": "TEDHKxCExwPy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 方策勾配法定理\n",
        "\n",
        "定義方法は、以下の 2 種類。\n",
        "\n",
        "- 平均報酬\n",
        "- 割引報酬和\n",
        "\n",
        "上記 2 種類の定義に対応して、 $ Q(s, a) $ の定義を行う。<br>\n",
        "すると、方策勾配法定理が成り立つ。\n",
        "\n",
        "$\n",
        "\\theta^{(t+1)} = \\theta^{(t)} + \\epsilon \\nabla J (\\theta)\n",
        "$\n",
        "の、 $ \\nabla J (\\theta) $ 部分は、\n",
        "\n",
        "ある行動をとるときの報酬を、以下とすると、\n",
        "\n",
        "$$\n",
        "\\pi_{\\theta}\n",
        "( a \\mid s)\n",
        "Q^{\\pi} (s, a)\n",
        "$$\n",
        "\n",
        "$ A $ をエージェントがとる全ての行動とすると、\n",
        "\n",
        "$$\n",
        "\\nabla_{\\theta} J (\\theta)\n",
        "=\n",
        "\\nabla_{\\theta}\n",
        "\\sum_{a \\in A}\n",
        "\\pi_{\\theta}\n",
        "( a \\mid s)\n",
        "Q^{\\pi} (s, a) \\\\\n",
        "=\n",
        "\\mathrm{E}_{\\pi_{\\theta}} [\n",
        "  (\n",
        "    \\nabla_{\\theta} \\log{\\pi_{\\theta}}\n",
        "    ( a \\mid s)\n",
        "    Q^{\\pi} (s, a)\n",
        "  )\n",
        "]\n",
        "$$\n"
      ],
      "metadata": {
        "id": "WA98K7Hqx41K"
      }
    }
  ]
}