{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "day-4-6-object-detection-segmentation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPklomTfpulJZNgoKmC4uYQ"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Adnxs5kDc2pd"
      },
      "source": [
        "# (day4) Section 6: 物体検知 - セグメンテーション\n",
        "\n",
        "本書は、「深層学習後編（day4）レポート」の、「Section 6: 物体検知 - セグメンテーション」についてのものです。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_jMHYWudCh3"
      },
      "source": [
        "## 1. 要点まとめ\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<!-- - page. 124\n",
        "\n",
        "### Introduction\n",
        "#### 広義の物体認識タスク\n",
        "### 代表的データセット\n",
        "### 評価指標\n",
        "#### 分類問題における評価指標\n",
        "#### 閾値変化に対する振る舞い\n",
        "#### IOU: Intersection over Union\n",
        "#### Precision/Recall\n",
        "#### AP: Average Precision\n",
        "#### mAP mean Average Precision\n",
        "#### FPS: Flames per Second\n",
        "### 物体検知の大枠\n",
        "#### マイルストーン: 深層学習以降の物体検知\n",
        "#### 物体検知のフレームワーク\n",
        "#### One-stage detector, Two-stage detector\n",
        "### SSD: Single Shot Detector\n",
        "#### SSD in a nutshell\n",
        "#### VGG16 のネットワーク図\n",
        "#### SSD のネットワークアーキテクチャ\n",
        "#### 特徴マップからの出力\n",
        "#### SSD のデフォルトボックス数\n",
        "#### その手の工夫\n",
        "#### 損失関数\n",
        "### Semantic Segmentation の概略\n",
        "#### 要点\n",
        "#### VCG16\n",
        "#### Up-samplng と Pooling\n",
        "#### NN における受容野\n",
        "#### FCN ( Fully Convolutional Network ) の基本\n",
        "#### Deconvolution / Transposed convolution\n",
        "#### 輪郭情報の補完\n",
        "#### U-Net\n",
        "#### DeconvNet & SegNet\n",
        "#### Unpooling\n",
        "#### Dilated Convolution -->\n"
      ],
      "metadata": {
        "id": "ZcRZlHZ-rYHj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 広義の物体認識タスク\n",
        "\n",
        "物体認識タスクの種類は、以下の通り。\n",
        "\n",
        "| 種類 | 出力 |\n",
        "| -- | -- |\n",
        "| 分類 | 画像1つの中のクラスラベル |\n",
        "| 物体検知 | Bounding Box |\n",
        "| 意味領域分割 | 各ピクセルのクラスラベル |\n",
        "| 個体領域分割 | 各ピクセルのクラスラベル |\n"
      ],
      "metadata": {
        "id": "B-vZwuBYOlDw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 代表的データセット\n",
        "\n",
        "物体検知の性能を評価する上で、基準となるデータセットが重要。\n"
      ],
      "metadata": {
        "id": "fB56pV8-QaD-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| データセット | クラス | Train + Val | Box / 画像 | Annotation |\n",
        "| -- | -- | -- | -- | -- |\n",
        "| VOC12 | 20 | 11,540 | 1.1 | Instance Annotation |\n",
        "| ILSVRC17 | 200 | 476,668 | 2.4 | |\n",
        "| MS COCO18 | 80 | 123,287 | 7.3 | Instance Annotation |\n",
        "| OICOD18 | 500 | 1,743,042 | 7.0 | Instance Annotation |\n",
        "\n",
        "クラス数が大きいことが有利であるわけではない。\n",
        "( 意味の少ない分類も多い )\n"
      ],
      "metadata": {
        "id": "N5Vf2f-JQwho"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 評価指標\n",
        "\n"
      ],
      "metadata": {
        "id": "mL9a7z9DR9Q8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 分類問題における評価指標\n",
        "\n"
      ],
      "metadata": {
        "id": "0nAEKFECPgUq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Confusion Matrix\n",
        "\n",
        "| 真値 - 予測 | Positive | Negative |\n",
        "| -- | -- | -- |\n",
        "| Positive | True<br/>Positve | False<br/>Negative |\n",
        "| Negative | False<br/>Positve | True<br/>Negative |\n",
        "\n",
        "$\n",
        "Precision = \\frac{TP}{TP + FP}\n",
        "$\n",
        "\n",
        "$\n",
        "Recall = \\frac{TP}{TP + FN}\n",
        "$\n",
        "\n",
        "confidence の閾値を変化させて、 Precision-Recall curve を描く。\n"
      ],
      "metadata": {
        "id": "SWM9GO2QP1Md"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 閾値変化に対する振る舞い\n",
        "\n",
        "- クラス分類\n",
        "\n",
        "- 物体検出<br/>\n",
        "  閾値 ( conf. の Threashold ) を上げると、\n",
        "  クラス分類の場合、サンプルの数は変わらない。\n",
        "  物体検出の場合、認識される数 ( BBox の数 ) 自体が減る。\n",
        "\n",
        "<!-- - TODO/Done: Precision-Recall curve との関連。 -->\n"
      ],
      "metadata": {
        "id": "lA_76nJoRKJk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### IOU: Intersection over Union\n",
        "\n",
        "クラスラベルと物体位置の予測精度を評価するためのもの。\n",
        "\n",
        "Confusion Matrix の要素から計算。\n",
        "\n",
        "$$\n",
        "IoU = \\frac{Area \\, of \\, Overlap}{Area \\, of \\, Union} \\\\\n",
        "= \\frac{TP}{TP + FP + FN}\n",
        "$$\n",
        "\n",
        "- NOTE: TN は、評価の範囲外なので、分母に含まれない。\n",
        "\n",
        "Jaccard 係数とも言う。\n",
        "\n",
        "- [Intersection over Union (IoU) for object detection - PyImageSearch](https://www.pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/)\n"
      ],
      "metadata": {
        "id": "FkvUnz5ASF97"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Precision/Recall\n",
        "\n",
        "$ conf \\geq 0.5, IoU \\geq 0.5 $ が評価対象。\n",
        "\n",
        "$ IoU \\gt Threashold $ で、 TP 、$ \\leq $ ならば FP\n",
        "\n",
        "- 入力 1 枚に対しての評価<br/>\n",
        "  複数のクラスを評価。<br/>\n",
        "  既に検知済みのクラスを重複して検知した場合は、 ( 検知が正しいとしても ) FP とする。\n",
        "\n",
        "- クラス単位の評価<br/>\n",
        "  画像は複数で、あるクラスについて評価。 ( 講義資料の例では、「人」 1 クラス )<br/>\n",
        "  ここでも既に検出済みは、 FP 。<br/>\n",
        "  また、 Recall において、 1 回も検出されなかった Ground-Truth の画像は、 FN で計数。\n",
        "\n",
        "\n",
        "<!-- - TODO/Done: クラス単位の FN 検知に対応する表の行? -->\n"
      ],
      "metadata": {
        "id": "8J_5gNwHzsCe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### AP: Average Precision\n",
        "\n",
        "対象のクラスラベルを固定にして、\n",
        "\n",
        "conf. の閾値を $ \\beta $ とするとき、\n",
        "\n",
        "$$\n",
        "Recall = R(\\beta), Precision = P(\\beta)\n",
        "$$\n",
        "\n",
        "Precision-Recall curve は、\n",
        "\n",
        "$$\n",
        "P = f(R)\n",
        "$$\n",
        "\n",
        "Average Precision は、\n",
        "\n",
        "$$\n",
        "AP\n",
        "=\n",
        "\\int_{0}^{1} P(R) d R\n",
        "$$\n",
        "\n",
        "Precision-Recall curve の PB 曲線の、\n",
        "下側面積を意味する。\n"
      ],
      "metadata": {
        "id": "W9meNV_kz3zu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### mAP mean Average Precision\n",
        "\n",
        "クラスごとに計算された AP の算術平均。\n",
        "\n",
        "クラス数が C のとき、\n",
        "\n",
        "$$\n",
        "mAP = \\frac{1}{C} \\sum_{i=1}^C AP_i\n",
        "$$\n"
      ],
      "metadata": {
        "id": "VBkWYPGcHIt1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### FPS: Flames per Second\n",
        "\n",
        "検出速度の指標。\n",
        "\n",
        "横軸を FPS 、縦軸を AP にして、プロットする。\n",
        "\n",
        "FPS を大きく ( 検知速度を速める ) すると、\n",
        "精度 ( AP ) が低下する傾向にある。\n"
      ],
      "metadata": {
        "id": "g2FPy2Nrz7Ej"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 物体検知の大枠\n"
      ],
      "metadata": {
        "id": "vPiCPaZOJalq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### マイルストーン: 深層学習以降の物体検知\n",
        "\n",
        "- 2012 AlexNet<br/>\n",
        "  SIFT: Scale Invariant Feature Transform から、<br/>\n",
        "  DCNN: Deep Convolutional Neural Network が主流に。\n",
        "\n",
        "- [元論文](https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)\n"
      ],
      "metadata": {
        "id": "RkfApHdiJmFE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 物体検知のフレームワーク\n",
        "\n",
        "以下の 2 種類。近年は、 One-stage detector が主流。\n",
        "\n",
        "| 種類 | 候補領域の検出</br>クラス推定 | 精度 | 計算量 | 推論 | \n",
        "| -- | -- | -- | -- | -- |\n",
        "| Two-stage detector | 別々に行う | 相対的に高い | 相対的に大きい | 遅い |\n",
        "| One-stage detector | 同時に行う | 相対的に低い | 相対的に小さい | 早い |\n",
        "\n",
        "Two-stage detector では、特徴マップから候補領域の検出のネットワークを経由することに対して、分類のネットワークに至る野に対して、<br/>\n",
        "One-stage detector では、1 つの Detection ネットワークで構成される。\n"
      ],
      "metadata": {
        "id": "2OuFdWJoL8uC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SSD: Single Shot Detector\n",
        "\n",
        "具体的な One-stage detector のアルゴリズムの一つとして、 SSD がある。\n",
        "\n",
        "<!-- - TODO/Done: SSD は、 One-stage detector のことか? -->\n"
      ],
      "metadata": {
        "id": "fodr9swLqmwJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### SSD in a nutshell\n",
        "\n",
        "1. 適当な位置、大きさの、Default BOX を用意して、検出したい物体を囲む。 \n",
        "\n",
        "1. Default BOX を検出したい物体に合せるように変形して、 Conf. を出力する。<br/>\n",
        "  この繰り返しで、学習を進める。\n"
      ],
      "metadata": {
        "id": "S1PRR4V1qnUd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### VGG16 のネットワーク図\n",
        "\n",
        "- [原著論文](https://arxiv.org/pdf/1409.1556.pdf)<br/>\n",
        "  > 2.1 ARCHITECTURE\n",
        "\n",
        "CN と FN の合計で、16層のアーキテクチャ。\n"
      ],
      "metadata": {
        "id": "rFZY6JwEsyYi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### SSD のネットワークアーキテクチャ\n",
        "\n",
        "- [原著論文](https://arxiv.org/pdf/1512.02325.pdf)<br/>\n",
        "  以下の、上の図。<br/>\n",
        "  > 2.1 Model<br/>\n",
        "  ...<br/>\n",
        "  > Fig. 2: A comparison between two single shot detection models: SSD and YOLO\n",
        "\n",
        "- SSD は、 VGG16 を元に構成。<br/>\n",
        "  >  We use the VGG-16 network as a base, but other networks should also produce good results\n",
        "\n",
        "- 入力サイズにより、 SSD300、SSD512 と表記。<br/>\n",
        "  上の図は、 SSD300 。<br/>\n",
        "  > Figure 2 shows the architecture details of the SSD300 model.\n",
        "\n",
        "- VGG16 の FC ( \"three Fully-Connected (FC) layers\" の 2 層 ) を Conv 層に変更。<br/>\n",
        "  一番最後の FN も削除。\n",
        "\n",
        "- VGG16 の10層目に相当する Conv 層\n",
        "\n",
        "- SSD の特徴であるマルチスケール特徴マップ</br>\n",
        "  > multiple feature maps with different resolutions\n"
      ],
      "metadata": {
        "id": "hhhpzN46uRAI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 特徴マップからの出力\n",
        "\n",
        "引用元の以下。\n",
        "\n",
        "> 2 The Single Shot Detector (SSD)<br/>\n",
        "...<br/>\n",
        "> Fig. 1: SSD framework.\n",
        "\n",
        "> 2.1 Model<br/>\n",
        "...<br/>\n",
        "> Default boxes and aspect ratios\n",
        "\n",
        "- 出力サイズ `#Class + 4` の、オフセット項\n",
        "\n",
        "オフセット項は、 $ \\Delta x, \\Delta y, \\Delta w, \\Delta h $\n",
        "\n",
        "$$\n",
        "x = x_0 + 0.1 \\times \\Delta x \\times w_0 \\\\\n",
        "y = y_0 + 0.1 \\times \\Delta y \\times w_0 \\\\\n",
        "w = w_0 + 0.1 \\times \\Delta w \\times w_0 \\\\\n",
        "h = h_0 + 0.1 \\times \\Delta h \\times w_0\n",
        "$$\n",
        "\n",
        "特徴マップの各特徴量に、 k 個の Default Box を用意、<br/>\n",
        "さらに、特徴マップのサイズを、 m x n とすると、<br/>\n",
        "特徴マップの出力サイズは、 k x ( クラス数 + 4 (オフセット項の数) )\n"
      ],
      "metadata": {
        "id": "IILga4OTyL1s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### SSD のデフォルトボックス数\n",
        "\n",
        "- VGG-16 through Conv5 3 layer からの Default Bbox は、各特徴に 4 つ。\n",
        "\n",
        "- Classifier 以降の Default Bbox は、各特徴に 6 つ。<br/>\n",
        "  これは計算量の都合のため、 Default Box の数を調整したもの。\n",
        "\n",
        "> 3.1 PASCAL VOC2007<br/>\n",
        "> For conv4 3,\n",
        "conv10 2 and conv11 2, we only associate 4 default boxes at each feature map location<br/>\n",
        "...<br/>\n",
        ">  For all other layers, we put 6 default boxes as\n",
        "described in Sec. 2.2.\n",
        "\n",
        "各ネットワークの特徴マップ・サイズに、 ( VOC データセット ) クラス数 20 + 背景 1 として、\n",
        "`( #Class = 21 ) + 4` を掛けたものが、デフォルトボックスの数になる。\n"
      ],
      "metadata": {
        "id": "eL6bw_Nn2nu5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- アーキテクチャの図で、特徴マップのサイズで図の要素の大きさを表現している。<br/>\n",
        "  しかし、この大きさ ( 特徴マップのサイズ ) と、解像度は別である。<br/>\n",
        "  解像度が大きい層は、小さな物体を認識し、\n",
        "  解像度が小さい層は、大きな物体を認識する。\n"
      ],
      "metadata": {
        "id": "m9RsLVaM5s3W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### その他の工夫\n",
        "\n",
        "- non-maximum suppression (nms)<br/>\n",
        "  複数の BBox が重なってしまっている場合への対応。<br/>\n",
        "  IoU が同じであれば、同じ対象を選択している可能性が高いため、 Conf. の高い BBox を残す。<br/>\n",
        "  > By using a confidence threshold of 0.01, we can filter out most boxes. \n",
        "\n",
        "- Hard negative mining<br/>\n",
        "  ネガティブな BBox を消す。<br/>\n",
        "  学習時にデフォルトボックスをフィルター。<br/>\n",
        "  > Instead of\n",
        "using all the negative examples, we sort them using the highest confidence loss for each\n",
        "default box and pick the top ones so that the ratio between the negatives and positives is\n",
        "at most 3:1.\n",
        "\n",
        "その他に、重ならない様に default bonding box の位置と縦横比を決める、\n",
        "\"Choosing scales and aspect ratios for default boxes\" 、<br/>\n",
        "学習でより多様な訓練データを得るための、\n",
        "\"Data augmentation\" がある。\n"
      ],
      "metadata": {
        "id": "yboh2gIQ6aJ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 損失関数\n",
        "\n",
        "引用元の以下の箇所。\n",
        "\n",
        "> 2.2 Training<br/>\n",
        "...<br/>\n",
        "> Training objective\n",
        "\n",
        "confidence の損失と、検出位置の損失の、和。\n",
        "\n",
        "> The overall objective loss\n",
        "function is a weighted sum of the localization loss (loc) and the confidence loss (conf):\n",
        "\n",
        "- NOTE: [参考引用: Smooth L1 Loss](https://arxiv.org/pdf/1504.08083.pdf)<br/>\n",
        "  > 2.3. Fine-tuning for detection<br/>\n",
        "  ...<br/>\n",
        "  > Multi-task loss.\n",
        "\n",
        "$$\n",
        "smooth_{L1}(x) = \\begin{equation}\n",
        "\\begin{cases}\n",
        "0.5x^2 & if \\; x \\gt 0 \\\\\n",
        "|x| - 0.5 & otherwise,\n",
        "\\end{cases}\n",
        "\\end{equation}\n",
        "$$\n"
      ],
      "metadata": {
        "id": "mNygs-Cs8HEG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Semantic Segmentation の概略\n"
      ],
      "metadata": {
        "id": "qpRlz9ZF-xYE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 要点\n",
        "\n",
        "順伝播で推論、逆伝播で学習。\n",
        "Convolution + Pooling というとろは今までと同様。<br/>\n",
        "ただ、要約したクラス数から、分割した領域のサイズに復元するために、 \"Up-sampling\" の壁がある。\n",
        "\n",
        "例えば、 VGG16 の場合、 ( 最後の ) max pooling によって、 7 x 7 の解像度まで下がる。\n"
      ],
      "metadata": {
        "id": "dlkUIK6R-zOf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Up-samplng と Pooling - NN における受容野 -\n",
        "\n",
        "Pooling はある程度の大きさのある受容野を確保するためのもの。\n",
        "\n",
        "受容野を大きくするために、深い Conv 層と pooling + stride で対応、\n",
        "深い Conv 層は計算資源的に効率的でない。\n",
        "なので、 pooling + stride が受容野を広げるために必要。\n"
      ],
      "metadata": {
        "id": "zVh_6vU0_v9b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### FCN ( Fully Convolutional Network ) の基本\n",
        "\n",
        "VGG16 の最後の FNN を Conv 層に置き換えている。\n",
        "\n",
        "- [引用元](https://arxiv.org/pdf/1411.4038.pdf)<br/>\n",
        "  > Figure 2. Transforming fully connected layers into convolution\n",
        "layers enables a classification net to output a heatmap.\n",
        "\n",
        "\"tabby cat\" という特徴のヒートマップを得る。\n",
        "このヒートマップを up sampling で元の解像度に戻して、領域における分類を行う。\n",
        "\n",
        "<!-- - TODO/Done: 結論までの論理的な道筋が不十分。 -->\n"
      ],
      "metadata": {
        "id": "ec5jcwAIwNXh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Deconvolution / Transposed convolution\n",
        "\n",
        "- [vdumoulin/conv_arithmetic: A technical report on convolution arithmetic in the context of deep learning](https://github.com/vdumoulin/conv_arithmetic)<br/>\n",
        "  各種類の、畳み込みの様子のアニメーション。\n",
        "  - 対応するアニメーションは、<br/>\n",
        "    - [Transposed convolution animations](https://github.com/vdumoulin/conv_arithmetic#transposed-convolution-animations)<br/>\n",
        "      \"Padding, strides, transposed\"\n",
        "\n",
        "\n",
        "- NOTE: DECONVOLUTION<br/>\n",
        "  - https://arxiv.org/pdf/1905.11926.pdf<br/>\n",
        "    page. 2 の馬の画像の例。\n"
      ],
      "metadata": {
        "id": "V2LpItomDOhP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 輪郭情報の補完\n",
        "\n",
        "Pooling すると輪郭が失われていく。 ( ぼやける )\n",
        "これを低レイヤーの出力で保管する。\n",
        "( 低レイヤーの出力を element wise addition: 要素ごとに追加する )\n",
        "\n",
        "- [引用元](https://arxiv.org/pdf/1411.4038.pdf)<br/>\n",
        "  > Figure 3. Our DAG nets learn to combine coarse, high layer information with fine, low layer information.\n",
        "\n",
        "FFCN-8s でだいぶ輪郭が復元できていることが分かる。\n"
      ],
      "metadata": {
        "id": "-reQnVTvFwl2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### U-Net\n",
        "\n",
        "<!-- - TODO/Done: U-Net とは? 特徴? -->\n",
        "\n",
        "FCN と類似して、低レイヤーの出力を使用する例として、 U-Net がある。\n",
        "\n",
        "- [出典](https://arxiv.org/pdf/1505.04597.pdf)<br/>\n",
        "  > Fig. 1. U-net architecture\n",
        "\n",
        "Skip-connection として pooling 前の情報を使う。\n",
        "\n",
        "FCN の element wise addition と違い、チャネル方向に低レイヤーの出力を結合している。\n"
      ],
      "metadata": {
        "id": "J9VTKkycHsp3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### DeconvNet & SegNet\n",
        "\n",
        "Semantic Segmentaion のネットワークの例。\n",
        "\n",
        "どのネットワークも、解像度を低くして特徴を得た後、 up sampling で元の解像度に戻して個体領域分割の結果を得ている。\n",
        "\n",
        "- [Learning Deconvolution Network for Semantic Segmentation](https://arxiv.org/pdf/1505.04366.pdf)<br/>\n",
        "  > The deconvolution network is composed of deconvolution and unpooling layers, which identify pixel-wise\n",
        "class labels and predict segmentation masks.\n",
        "\n",
        "- [SegNet: A Deep Convolutional\n",
        "Encoder-Decoder Architecture for Image\n",
        "Segmentation](https://arxiv.org/pdf/1511.00561.pdf)<br/>\n",
        "  > \n",
        "\n",
        "<!-- - TODO/Done: ここも各 Net の特徴など聞く。 -->\n"
      ],
      "metadata": {
        "id": "grsopahoIn4N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Unpooling\n",
        "\n",
        "Deconvolution と異なったアプローチの up sampling 方法。\n",
        "\n",
        "Pooling 時に位置情報を保持しておき、\n",
        "up sampling の時に、保持していた位置情報を使う。\n",
        "位置情報と言うのは、例えば Max pooing ならば、どこが最大だったかの位置のこと。\n",
        "どこが最大だったかの情報を使って、 up sampling できる。\n",
        "\n",
        "- 引用元の、<br/>\n",
        "  > 3.2.1 Unpooling\n",
        "\n",
        "Pooling により、受容野の空間的な情報が失われる。\n",
        "これを元のサイズに再構築するのが、 Unpooling 層。\n",
        "\n"
      ],
      "metadata": {
        "id": "dbBgG5u2KWYX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Dilated Convolution\n",
        "\n",
        "pooling ではなく、 Conv 層で受容野を広げる手法。\n",
        "\n",
        "- [MULTI-SCALE CONTEXT AGGREGATION BY\n",
        "DILATED CONVOLUTIONS](https://arxiv.org/pdf/1511.07122.pdf)<br/>\n",
        "  > Figure 1: Systematic dilation supports exponential expansion of the receptive field without loss of\n",
        "resolution or coverage.\n",
        "\n",
        "$ i + 1 $ 番目の層のサイズを、以下 $ F_{i+1} $ に拡張できる。\n",
        "\n",
        "$ F_{i+1} = (2^{i+2} - 1) \\times (2^{i+2} - 1) $\n"
      ],
      "metadata": {
        "id": "hDCsVQKLLGoi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<!-- - TODO: 7 層必要になる計算の方法 -->\n"
      ],
      "metadata": {
        "id": "kVfEd2D3uP4J"
      }
    }
  ]
}