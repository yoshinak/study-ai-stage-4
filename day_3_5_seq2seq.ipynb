{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "day-3-5-seq2seq.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPIQJ6MoiIlc39GMhmoXep3"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Adnxs5kDc2pd"
      },
      "source": [
        "# (day3) Section 5: Seq2Seq\n",
        "\n",
        "本書は、「深層学習後編（day3）レポート」の、「Section 5: Seq2Seq」についてのものです。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_jMHYWudCh3"
      },
      "source": [
        "## 1. 要点まとめ\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<!-- - page. 97 - 128\n",
        "\n",
        "- Section 5: Seq2Seq\n",
        "  - seq2seq 全体図<br>\n",
        "    含む \"Seq2seq とは\"\n",
        "  - 5-1 Encoder RNN\n",
        "    - 全体像\n",
        "    - 処理手順\n",
        "  - 5-2 Decoder RNN\n",
        "    - 全体像\n",
        "    - 処理手順<br>\n",
        "      含む \"Decoder RNN の処理\"\n",
        "    - seq2seq 全体図(具体例)\n",
        "    - one-hot ベクトル\n",
        "  - 5-3 HRED\n",
        "    - seq2seq の課題<br>\n",
        "      含む \"HRED\"\n",
        "    - HRED の構造\n",
        "    - HRED の課題\n",
        "  - 5-4 VHRED\n",
        "  - 5-5 VAE\n",
        "    - 5-5-1 オートエンコーダ\n",
        "      - オートエンコーダ構造\n",
        "      - オートエンコーダ構造図\n",
        "    - 5-5-2 VAE\n",
        "      - VAE 構造 -->\n"
      ],
      "metadata": {
        "id": "FMgQtbvVDjmq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### seq2seq 全体図\n",
        "\n",
        "seq2seq は、 2 つの RNN を結合したもの。主に機械翻訳に使う手法。\n",
        "\n",
        "page. 98 の図、 1つ目 (左下) の RNN に、文章の単語を順々に入力する。\n",
        "$ h^{t=T^x} $ に順番にデータが蓄積される。\n",
        "これが、元の文章の文脈として蓄積される。\n",
        "図の中心の、「C 文脈」が蓄積された文脈のベクトル表現。\n",
        "\n",
        "\n",
        "この文脈を、 2 つ目の RNN (右上) に渡す。\n",
        "別の出力に作り替える。\n",
        "\n",
        "\n",
        "具体例としては、<br>\n",
        "1つ目の RNN で日本語で文脈を理解して、<br>\n",
        "2つ目の RNN で英語で表現する。\n",
        "\n",
        "\n",
        "1つ目の RNN を、エンコーダ、<br>\n",
        "2つ目の RNN を、デコーダと言う。\n",
        "\n",
        "この様なモデルを、 Encoder-Decoder モデルと言う。\n"
      ],
      "metadata": {
        "id": "bNYsejCLeaR5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Seq2seq は、\n",
        "Encoder-Decoder モデルの一種。\n"
      ],
      "metadata": {
        "id": "kreaWVTtgyqM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5-1 Encoder RNN\n"
      ],
      "metadata": {
        "id": "GVyujhdxhLQ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 全体像\n",
        "\n",
        "$$\n",
        "x^{t=1}, \\cdots, x^{t=T^x}\n",
        "$$\n",
        "\n",
        "と、文章の単語 ( トークン ) を取り込んで、\n",
        "\n",
        "$$\n",
        "x^{t=1} \\to h^{t=1}, \\cdots, x^{t=T^x} \\to h^{t=T^x}\n",
        "$$\n",
        "\n",
        "隠れ層に単語の意味が注入されていく。\n",
        "最終的に、 $ h^{t=T^x} $ に文章全体のベクトル表現が蓄積される。\n"
      ],
      "metadata": {
        "id": "h2q1cbYxhOQf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Taking\n",
        "\n",
        "文章を解析したトークンの一つ一つは、番号付け ( ID ) して、ベクトルの各 1 要素に対応付ける。\n",
        "例えば、1万語の文章ならば、1万のベクトル要素になる。\n",
        "\n",
        "この 1 つのベクトルのことを、 one-hot ベクトルと言う。\n"
      ],
      "metadata": {
        "id": "_hNPj0zWiRDZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Embedding\n",
        "\n",
        "one-hot ベクトルは大きいサイズなので、(先の例では、1万要素)<br>\n",
        "Embedding と言うものを行う。\n",
        "\n",
        "この Embedding 表現によって、ベクトルのサイズ ( 要素数 ) を数百程度にできる。\n",
        "\n",
        "Embedding では、機械学習を用いる。\n",
        "似たような単語では Embedding 表現が似る様に学習を進める。\n"
      ],
      "metadata": {
        "id": "EzqBETj4kGol"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "page. 110 の「演習チャレンジ」の、 `E: word embedding matrix` は、\n",
        "単語毎に対応する Embedding 表現を格納した行列のこと。\n",
        "\n",
        "`for w in words` の各単語 'w' ( one-hot vector ) と内積を取ると、その Embedding 表現を取得することができる。\n",
        "\n",
        "```python\n",
        "e = E.dot(w)\n",
        "```\n"
      ],
      "metadata": {
        "id": "sVNNr0d_sgw9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 処理手順\n",
        "\n",
        "1. 繰り返し\n",
        "  1. 入力 vec_n と前の hidden sate を入力として、hidden state を出力する。\n",
        "\n",
        "1. 最後の vec_n の入力に対する hidden state が final state.<br>\n",
        "  final state が入力した分の意味を表すベクトル ( thought vector )\n"
      ],
      "metadata": {
        "id": "BcAcZdwKkUaA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 事例<br>\n",
        "  自然言語の分野の最近では、 Google の BART で、以下の様にトークンをベクトルで扱っている。\n",
        "  - MLM: Masked Language Model<br>\n",
        "    ランダムに 1 つの単語を削除して、その単語を予測させる様に学習すると、文章全体で、類似の単語のベクトルが学習される。\n",
        "\n",
        "こういった単語のベクトル ( 特徴ベクトル ) を作成するには、自力で教師なし学習で獲得させることが重要。\n"
      ],
      "metadata": {
        "id": "rGh0wDlnlgY5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5-2 Decoder RNN\n",
        "\n",
        "最初の入力 $ h^{t=1} $ は、 Encoder RNN が作成した、文章の文脈表現。\n",
        "最終的な出力として、新たな文脈表現を作成する。\n",
        "( ここでは機械対話の例 )\n"
      ],
      "metadata": {
        "id": "cXNIo39ZnGDJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 処理手順\n",
        "\n",
        "<!-- 1. Encoder RNN の final state ( thought vector ) から、各 token の生成確立を出力する。\n",
        "  1. final state を Decoder RNN の initial state として設定する。 -->\n",
        "\n",
        "Encoder RNN の final state ( thought vector ) から、最初の1単語目を作成する。\n",
        "1単語目ができたら、次々と単語を予測していく。\n"
      ],
      "metadata": {
        "id": "mCRtgloJn-sK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decoder のときとは、逆方向に単語を作成する。 ( Detokenize )\n",
        "\n",
        "$$\n",
        "embedding \\to one-hot \\to ID \\to word\n",
        "$$\n"
      ],
      "metadata": {
        "id": "f6bwlaReqCJU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5-3 HRED ( Hierarchical Recurrent Encoder-Decoder )\n",
        "\n",
        "Seq2seq では、1つの文 ( 文脈表現 ) に対して、1 つの結果しか得られない。\n",
        "( 機械対話の例では、「一問一答」 )\n",
        "\n",
        "HRED は、1 つの文だはなく、過去の分も含めた文脈で、結果を得るための仕組み。\n",
        "\n",
        "- NOTE: 略語: HRED の全文の主典は以下。<br>\n",
        "  https://arxiv.org/pdf/1507.04808.pdf<br>\n",
        "  講義動画 5:29:20 で紹介された図は、こちらの page. 3 上部のものである理解。\n",
        "\n",
        "Seq2seq に Context RNN を追加したもの。\n",
        "Context RNN は、これまでの会話のコンテキスト ( 文脈 ) 全体を表すベクトルに蓄積。\n",
        "( context hdden state のことと理解 )\n",
        "これを次の文の、 seq2seq に引き継ぐことで、会話全体の文脈を維持する。\n",
        "\n"
      ],
      "metadata": {
        "id": "ECspPMvct0QQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "HRED にも課題がある。文脈の流れと言うより、確率的にありがちな回答を学習してしまう。(会話の中で意味が乏しいものに)\n"
      ],
      "metadata": {
        "id": "A8KAK_-ezSAN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5-5 VAE\n",
        "\n",
        "- NOTE: 講義動画より、 \"5-4 VHRED\" より先に、こちらを進める。<br>\n",
        "  ( VRED の前提知識が、 VAE である理解 )\n"
      ],
      "metadata": {
        "id": "oMvCvvMl0ay9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5-5-1 オートエンコーダ\n",
        "\n",
        "オートエンコーダの構造は、以下の通り。\n",
        "\n",
        "| 構造 | 説明 | \n",
        "| --- | --- |\n",
        "| 入力 | |\n",
        "| Encoder | 入力のデータよりも次元削減したデータ z を出力する。<br> Decoder が戻しやすいように学習する。<br>これは効率の良い次元削減したデータの作成を学習することを意味する。 |\n",
        "| z ( 洗剤変数 ) |  |\n",
        "| Decoder | z を入力にして、出力が Encoder の入力と一致するように学習する。 |\n",
        "| 出力 | |\n",
        "\n",
        "オートエンコーダは、教師なし学習であり、上記の入力である訓練データのみを使用する。\n"
      ],
      "metadata": {
        "id": "xmqHk0Gv9s69"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5-5-2 VAE ( Variational AutoEncoder )\n",
        "\n",
        "VAE の構造としては、オートエンコーダと同様に、以下の通り。\n",
        "z ( 洗剤変数 ) が異なる。 ( 通常のオートエンコーダでは、どの様な状態か分からない )\n",
        "\n",
        "| 構造 | 説明 | \n",
        "| --- | --- |\n",
        "| 入力 | |\n",
        "| Encoder | |\n",
        "| z ( 洗剤変数 ) | 確率分布 $ z \\sim \\mathcal{N}(0, 1) $ ( 標準正規分布 ) を仮定したの。<br> L2 ノルムで正規化する。 |\n",
        "| Decoder | |\n",
        "| 出力 | |\n",
        "\n",
        "元のデータの類似度が、潜在変数にも反映させるようにしたいため、\n",
        "元のデータと同じばらつきになるように、確率分布を適用する。\n",
        "通常のオートエンコーダより、汎用性を抽出できるようにしたもの。\n"
      ],
      "metadata": {
        "id": "47Vmr4MZ-rUF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "より汎用的な特徴を得るための学習の工夫として、 Encoder の出力部分と、 Decoder の入力にさらにノイズを加える手法がある。\n",
        "NN のドロップアウトの様な手法。\n"
      ],
      "metadata": {
        "id": "8IruFABGFdul"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5-4 VHRED\n",
        "\n",
        "HRED に VAE を適用したものである。\n"
      ],
      "metadata": {
        "id": "snSH_oG9F6zV"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dn0nkaSx8-Em"
      },
      "source": [
        "## 2. 確認テスト\n",
        "\n",
        "以降の \"page. \" は、講義資料のページの番号です。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### page. 109\n",
        "\n",
        "本要点まとめで整理した内容より、 (2) 。\n",
        "\n",
        "- (1): 双方向 RNN の説明。\n",
        "- (3): 文全体の表現ベクトルは作成するが、隣接単語からの表現ベクトルではなく、\n",
        "  Encoder RNN による時間的順序の学習をしている理解。\n",
        "- (4): GRU\n"
      ],
      "metadata": {
        "id": "Rt2LNSKoc3c9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### page. 119\n",
        "\n",
        "- HRED: seq2seq に、今までの文の文脈 ( Context RNN ) を加えて、会話の文脈で結果を作成できるようにしたもの。\n",
        "\n",
        "- VHRED: HRED に対して、結果により汎用性を高めるための VAE を適用したもの。\n"
      ],
      "metadata": {
        "id": "WgFCZPA0dngq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### page. 128\n",
        "\n",
        "通常のオートエンコーダでは不明な状態である洗剤変数に対して、確率分布を導入したもの。\n"
      ],
      "metadata": {
        "id": "MZOAy8YgeILq"
      }
    }
  ]
}