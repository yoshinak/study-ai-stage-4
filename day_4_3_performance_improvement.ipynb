{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "day-4-3-performance-improvement.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMioy07pI/u1+76a3cRcEjf"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Adnxs5kDc2pd"
      },
      "source": [
        "# (day4) Section 3: 軽量化 - 高速化技術\n",
        "\n",
        "本書は、「深層学習後編（day4）レポート」の、「Section 3: 軽量化 - 高速化技術」についてのものです。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_jMHYWudCh3"
      },
      "source": [
        "## 1. 要点まとめ\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "本章では、「分散深層学習」で学習自体の高速化を、<br/>\n",
        "「モデルの軽量化」でモデル自体を高速に学習可能とするための方法を扱う。\n"
      ],
      "metadata": {
        "id": "DBIEXqyV8cxR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<!-- - page. 37 - 74\n",
        "\n",
        "- 分散深層学習\n",
        "  - データ並列化\n",
        "    - 同期型\n",
        "    - 非同期型\n",
        "  - モデル並列化\n",
        "  - GPU による高速化\n",
        "    - 処理装置の種類\n",
        "    - GPGPU 開発環境\n",
        "- モデルの軽量化\n",
        "  - 量子化 ( Quantization )<br/>\n",
        "    表にする。\n",
        "    - 利点と欠点\n",
        "      - 利点\n",
        "        - 計算の高速化\n",
        "        - 省メモリ化\n",
        "      - 欠点\n",
        "        - 精度の低下<br/>\n",
        "          含む \"極端な量子化\"\n",
        "  - 蒸留 ( Distillation )<br/>\n",
        "    概要、モデルの簡約化\n",
        "    - 教師モデルと生徒モデル\n",
        "    - 蒸留の利点 ( 効果 )\n",
        "  - Pruning ( 剪定 ) <br/>\n",
        "    - 計算の高速化\n",
        "    - ニューロンの削減<br/>\n",
        "      含む \"ニューロン数と精度\" -->\n"
      ],
      "metadata": {
        "id": "0Efy8WkCObpr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 分散深層学習\n",
        "\n",
        "計算資源が増大する傾向にある深層学習を行うため、複数のワーカーを使用し、並列で学習を行う。\n",
        "\n",
        "並列化の技術は、以下の通り。\n",
        "\n",
        "- データ並列化\n",
        "- モデル並列化\n",
        "- GPU による高速化\n"
      ],
      "metadata": {
        "id": "00ZO_zgT7uJl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### データ並列化\n",
        "\n",
        "データを分割して、各ワーカーに並列で学習させる。\n",
        "元のモデルを親モデル、コピーして各ワーカーで学習させるモデルを子モデルと言う。\n",
        "\n",
        "各ワーカーが子モデルで学習した結果を、\n",
        "親モデルに更新する方法によって、以下の 2 種類に分類される。\n",
        "\n",
        "- 同期型<br/>\n",
        "  全ワーカーの学習を待ち、全てのパラメータの勾配が産出されたら、\n",
        "  親モデルのパラメータに更新する。\n",
        "- 非同期型<br/>\n",
        "  各ワーカーが非同期に、独立して学習を進める。<br/>\n",
        "  学習された子モデルを、パラメータのやり取りをするパラメータサーバに、\n",
        "  Push して、<br/>\n",
        "  学習を始めるときは、改めてパラメータサーバから最新の子モデルを\n",
        "  Pop して学習を進める。\n",
        "\n",
        "更新を管理しやすく、精度が良い傾向にある同期型を使うことが多いが、<br/>\n",
        "ワーカーを配置するデバイスの種類など、ケースによって使い分ける。\n"
      ],
      "metadata": {
        "id": "MhONnYBb9LlF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### モデル並列化\n",
        "\n",
        "親モデルを分割して、各ワーカーで並列で学習する。\n",
        "\n",
        "分割のしかたは、以下の 2 通り。\n",
        "\n",
        "- 層の区切りで分割\n",
        "- ネットワークの分岐から分割<br/>\n",
        "  近年はこの場合が多い。\n",
        "\n",
        "モデルのパラメータ数が多いほど、効率が良い。\n",
        "ネットワークなどの、ワーカー間のオーバーヘッドを相殺するだけのモデルの大きさがあるとメリットが大きくなる。\n"
      ],
      "metadata": {
        "id": "MZC5vOyx_TqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### GPU による高速化\n",
        "\n",
        "演算処理装置に、並列処理が得意で行列計算に向いている GPU を利用して高速化する。\n",
        "\n",
        "GPU の元々の使用目的は、グラフィック演算。グラフィック以外の用途で使用される GPU のことを、 GPGPU ( General-purpose on GPU ) と言う。\n"
      ],
      "metadata": {
        "id": "lDUMpyjIAKCR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPGPU を使用するための開発環境には、 CUDA と OpenCL がある。\n",
        "\n",
        "現時点では、 OpenCL はほとんど使用されず、 NVIDIA 社製 GPU において Deep Learning 用に提供されている CUDA を使用することが多い。\n"
      ],
      "metadata": {
        "id": "z_CW4-2zBCDR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### モデルの軽量化\n",
        "\n",
        "モデルの精度を維持して、パラメータや演算回数を低減する手法。\n",
        "モバイル、 IoT 機器で有効。\n",
        "\n",
        "モデルの軽量化手法は、以下の通り。\n",
        "\n",
        "- 量子化 ( Quantization )\n",
        "- 蒸留 ( Distillation )\n",
        "- Pruning ( 剪定 )\n"
      ],
      "metadata": {
        "id": "7fVbyJidBaDd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 量子化 ( Quantization )\n",
        "\n",
        "パラメータの精度を、 64 bit 浮動小数点から 32 bit 、または 16 bit の下位の精度に下げること。\n",
        "\n",
        "大量のパラメータにおいて、パラメータの精度を下げるとその分のデータ量差削減となり、\n",
        "メモリ使用量、演算量を低減できる。\n"
      ],
      "metadata": {
        "id": "c_iFcddTB7ZH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "量子化の利点と欠点は、以下の通り。\n",
        "\n",
        "- 利点\n",
        "  - 計算の高速化<br/>\n",
        "    倍精度演算 ( 64bit ) から、単精度演算 ( 32 bit ) にすることで、\n",
        "    約 2 倍に高速化できる。\n",
        "  - 省メモリ化<br/>\n",
        "    下げた bit 数分、メモリを節約できる。 ( 16 / 64 = 1 / 4 )\n",
        "\n",
        "- 欠点\n",
        "  - 精度の低下<br/>\n",
        "    精度を下げた分、浮動小数点の有効桁数が減るので、少数の有効桁が小さくなる。\n",
        "    結果、モデル ( 重み ) の表現力が低下する。\n",
        "\n",
        "実際問題としては、倍精度を単精度にしてもほぼ精度は変わらない。\n"
      ],
      "metadata": {
        "id": "rl7qppejDgzp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 蒸留 ( Distillation )\n",
        "\n",
        "精度の高い大きなモデルの知識 ( 学習結果の重み ) を使って、軽量な ( 小さな ) モデルを作成する。\n"
      ],
      "metadata": {
        "id": "ZCYcqrdyCA1N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "教師モデルと生徒モデルを使う。\n",
        "\n",
        "- 教師モデル<br/>\n",
        "  精度の高い大きなモデル\n",
        "- 生徒モデル<br/>\n",
        "  教師モデルの知識を使って学習させる、小さなモデル\n"
      ],
      "metadata": {
        "id": "VC-2vcKjGgfd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 教師モデルと生徒モデル ( 生徒モデルの学習方法 )\n",
        "\n",
        "- 教師モデルの重みは更新しない。 ( 学習が終わったお手本 )\n",
        "- 教師モデルと生徒モデルの両方に訓練データを入力する。\n",
        "- 両方の出力と訓練データの正解の誤差を得て、<br/>\n",
        "  生徒モデルの重みへ更新する。\n"
      ],
      "metadata": {
        "id": "4IzKP26XHAT1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- NOTE: Hints Training\n",
        "  - https://arxiv.org/pdf/1412.6550.pdf<br/>\n",
        "    \"Figure 1: Training a student network using hints\" より。\n",
        "\n",
        "教師モデルに、教師のヒントとしての $ W_{Hint} $ の重みの中間層を、\n",
        "生徒モデルに、指導を受ける $ W_{Guided} $ の重みの中間層を持つ。\n",
        "\n",
        "全体 ( 蒸留 ) の誤差とは別に、<br/>\n",
        "教師ヒントの中間層と、指導を受ける生徒の中間層の出力の誤差を、\n",
        "指導を受ける $ W_{Guided} $ の重みに更新しながら、学習を進める。\n"
      ],
      "metadata": {
        "id": "OwqUp5T1KPFy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Pruning ( 剪定 )\n",
        "\n",
        "モデルの精度に寄与が少ないニューロンを削減する。\n",
        "( 不要な枝: ニューロンを剪定する )\n",
        "\n",
        "削除したニューロンの分、モデルを軽量化できて、結果、高速化が見込まれる。\n",
        "\n",
        "page. 73 左表、 1 行目の様に約半分のパラメータにしても、91.66 % の精度か出ている。\n",
        "\n",
        "- NOTE: 元の精度\n",
        "  - https://db-event.jpn.org/deim2017/papers/62.pdf<br/>\n",
        "  page. 7 <br/>\n",
        "  > CaffeNet のパラメータ削減前の Oxford\n",
        "102 category flower dataset のテストデータにおける認識精度\n",
        "（top-1）は 91.82%\n",
        "\n",
        "パラメータを半分にしても、 0.16 % しか精度は落ちていない。\n",
        "\n",
        "上記引用元より、パラメータの削減には、以下の 2 種類がある。\n",
        "\n",
        "> 結合単位でのパラメータの削減\n",
        "\n",
        "ユニット ( ニューロン ) 間の結合を減らす。\n",
        "\n",
        "> ユニット単位でのパラメータの削減\n",
        "\n",
        "ユニット ( ニューロン ) を減らす。\n",
        "\n",
        "ネットワークによって、どちらが有効か異なるとのこと。\n",
        "先の CaffeNet は、「ユニット単位でのパラメータの削減」によるもの。\n"
      ],
      "metadata": {
        "id": "PCEmuGBUCEox"
      }
    }
  ]
}