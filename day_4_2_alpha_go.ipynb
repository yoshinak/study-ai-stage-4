{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "day-4-2-alpha-go.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMIOxT3UxKWAZsFVeZDuPxx"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Adnxs5kDc2pd"
      },
      "source": [
        "# (day4) Section 2: AlphaGo\n",
        "\n",
        "本書は、「深層学習後編（day4）レポート」の、「Section 2: AlphaGo」についてのものです。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_jMHYWudCh3"
      },
      "source": [
        "## 1. 要点まとめ\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- page. 17 - 36\n"
      ],
      "metadata": {
        "id": "0Efy8WkCObpr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "強化学習が注目を集めたきっかけとなったもの。\n"
      ],
      "metadata": {
        "id": "O0Db11dAXRb7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<!-- - page. 4 - 16\n",
        "\n",
        "- 概要\n",
        "- イメージ\n",
        "- 応用例\n",
        "- 探索と利用のトレードオフ\n",
        "  - 不完全な知識\n",
        "  - 探索と利用\n",
        "- 強化学習イメージ ( 構成要素 )\n",
        "- 強化学習の差分 ( 他の学習との違い )\n",
        "- 強化学習の歴史 ( 進展のきっかけとなった 2 つの手法の組み合わせ )\n",
        "- 価値関数\n",
        "- 方策関数\n",
        "- 方策勾配法 -->\n"
      ],
      "metadata": {
        "id": "9wtHxCTMZ_zu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Alpha Go (Lee)\n",
        "\n",
        "以下の 2 つの 畳み込みニューラルネットワークを使用している。\n",
        "\n",
        "- Policy Net: 方策関数\n",
        "- Value Net: 行動価値関数\n"
      ],
      "metadata": {
        "id": "XRq70EBHXa1y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### PolicyNet: 方策関数\n",
        "\n",
        "エージェントがどこに打つかを決める。\n",
        "\n",
        "- 入力<br/>\n",
        "  盤面特徴入力。 19 x 19 のマス、 48 チャンネル。<br/>\n",
        "  ( 例えるなら、 19 x 19 の画像の入力の様なもの )<br/>\n",
        "  48 チャンネルの内訳は、 page. 20\n",
        "\n",
        "- 出力<br/>\n",
        "  19 x 19 の、そのマスが最善手である確率。\n"
      ],
      "metadata": {
        "id": "1Fj4tAzMXrDO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- NOTE: 2 次元のデータを扱う時は、畳み込み NN で、畳み込んでプーリングする手法が、定石である。"
      ],
      "metadata": {
        "id": "aXUNzypmZLi1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ValueNet: 行動価値関数\n",
        "\n",
        "- 入力<br/>\n",
        "  盤面特徴入力。 19 x 19 のマス、 (手番が増えて) 49 チャンネル。<br/>\n",
        "\n",
        "- 出力<br/>\n",
        "  勝率を -1 - 1 の範囲で表したもの。\n",
        "\n",
        "2 次元から、1次元にして ( flatten ) 出力を得ている。\n"
      ],
      "metadata": {
        "id": "NkFu58FbXlZ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Alpha Go の学習\n"
      ],
      "metadata": {
        "id": "ChHgj99mbk_8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 強化学習による PolicyNet の学習\n",
        "\n",
        "学習した PolicyNet を、 PolicyPool に蓄積する。\n",
        "\n",
        "PolicyPool からランダムに PolicyNet を選択して、 ( PolicyNet が相手ではない) 対局シミュレーションを繰り返す。"
      ],
      "metadata": {
        "id": "aFiX038Lby4l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### RollOutPolicy ( 学習の効率化 )\n",
        "\n",
        "特徴を削減して、特徴を全て使用した PolicyNet の 1,000 倍の速さで学習するモデルを採用。\n",
        "\n",
        "\"強化学習による PolicyNet の学習\" を行う前の、\n",
        "PolicyNet の初期の学習である教師あり学習で使用する。\n",
        "\n",
        "学習の訓練データには、人間対人間の対局の記録を使用。\n"
      ],
      "metadata": {
        "id": "JsgpiiXSc1Fn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 強化学習による ValueNet の学習\n",
        "\n",
        "モンテカルロ木探索を使用。\n",
        "価値関数 ( Q関数 ) を学習、更新するための手法。\n",
        "探索には、軽量な RollOutPolicy を使用する。\n"
      ],
      "metadata": {
        "id": "NjyJoXpceROF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Alpha Go Zero\n",
        "\n",
        "違いは以下。\n",
        "\n",
        "- 教師あり学習は使用しない。強化学習のみ。\n",
        "\n",
        "- 特徴は、石の配置のみ。\n",
        "\n",
        "- PolicyNet と ValueNet を 1 つの NN に。\n",
        "\n",
        "- Residual Net ( 画像の識別によく使用される ) を採用。\n",
        "\n",
        "- モンテカルロ木探索から RollOut をなくした。\n"
      ],
      "metadata": {
        "id": "fwKC254-fBY3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PolicyValueNet ( = PolicyNet + ValueNet )\n",
        "\n",
        "中間の ResidulBlock の次から、 2 つの出力に枝分かれしている。\n"
      ],
      "metadata": {
        "id": "Lyysbzs1gDTi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ResidulBlock\n",
        "\n",
        "ネットワークを通らずに、出力にショートカットするルートを作る。\n",
        "\n",
        "ネットワークが深くなると、勾配消失問題が生じやすい。\n",
        "ネットワークの深さを抑える効果がある。\n",
        "\n",
        "ネットワークを経由しないと、また別のネットワークとして認識され、学習に多様性が生まれる。 ( アンサンブル効果 )\n"
      ],
      "metadata": {
        "id": "sUM9sY1NghNe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Residul Network の派生形 ( Alpha Go Zero における工夫 )\n",
        "\n",
        "- Rsidual Block\n",
        "  - Bottleneck\n",
        "  - PreActivation\n",
        "\n",
        "- Network 構造\n",
        "  - WideResNet\n",
        "  - PyramidNet\n"
      ],
      "metadata": {
        "id": "U2kSsReDht-G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- NOTE: 重要で基本的な手法 4 選\n",
        "  - 畳み込み\n",
        "  - Pooling\n",
        "  - RNN\n",
        "  - Attention\n"
      ],
      "metadata": {
        "id": "i-kKxRjXim4C"
      }
    }
  ]
}